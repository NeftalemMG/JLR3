%%writefile main_combined.py

import triton_fix
import os
import sys

import triton

# Create symlink if it doesn't exist
if not os.path.exists('/tmp/cuda_lib/libcuda.so'):
    os.makedirs('/tmp/cuda_lib', exist_ok=True)
    if os.path.exists('/usr/local/nvidia/lib64/libcuda.so.1'):
        try:
            os.symlink('/usr/local/nvidia/lib64/libcuda.so.1', '/tmp/cuda_lib/libcuda.so')
            print(" Created libcuda.so symlink")
        except FileExistsError:
            pass

# Set environment variables
os.environ['TRITON_LIBCUDA_PATH'] = '/tmp/cuda_lib'
os.environ['LD_LIBRARY_PATH'] = '/tmp/cuda_lib:/usr/local/nvidia/lib64:/usr/local/cuda-12.5/compat:' + os.environ.get('LD_LIBRARY_PATH', '')

print(f" TRITON_LIBCUDA_PATH = {os.environ['TRITON_LIBCUDA_PATH']}")
print(f" Symlink exists: {os.path.exists('/tmp/cuda_lib/libcuda.so')}\n")


import torch
import numpy as np
import time
from mnist_loader import load_mnist_dataset
from model import NetworkConfig, CNNModel
from training_ops import train_step, evaluate_model
from forward_pass import forward_pass
from triton_utils import CudaTimer

MODE_NAMES = [
    "Mode 0: ReLU+LayerNorm (Non-fused - 2 kernels)",
    "Mode 1: GELU+LayerNorm (Fused - 1 kernel)",
    "Mode 2: EvoNorm-B0 (Fused - 1 kernel)",
    "Mode 3: Swish+LayerNorm (Fused - 1 kernel)"
]

def calculate_memory_throughput(batch_size, num_channels, time_ms):
    """Calculate memory throughput in GB/s"""
    IMAGE_SIZE = 28
    CONV_OUT_H = 28
    CONV_OUT_W = 28
    POOL_OUT_H = 14
    POOL_OUT_W = 14
    NUM_CLASSES = 10
    KERNEL_SIZE = 3
    
    conv_output_size = batch_size * num_channels * CONV_OUT_H * CONV_OUT_W
    pool_output_size = batch_size * num_channels * POOL_OUT_H * POOL_OUT_W
    fc_input_dim = num_channels * POOL_OUT_H * POOL_OUT_W
    
    bytes_read = 0
    bytes_written = 0
    
    bytes_read += batch_size * IMAGE_SIZE * IMAGE_SIZE * 4
    bytes_read += num_channels * 1 * KERNEL_SIZE * KERNEL_SIZE * 4
    bytes_written += conv_output_size * 4
    bytes_read += conv_output_size * 4
    bytes_read += num_channels * 2 * 4
    bytes_written += conv_output_size * 4
    bytes_read += conv_output_size * 4
    bytes_written += pool_output_size * 4
    bytes_read += pool_output_size * 4
    bytes_read += NUM_CLASSES * fc_input_dim * 4
    bytes_written += batch_size * NUM_CLASSES * 4
    bytes_read += batch_size * NUM_CLASSES * 4
    bytes_written += batch_size * NUM_CLASSES * 4
    
    total_bytes = bytes_read + bytes_written
    time_seconds = time_ms / 1000.0
    return (total_bytes / 1e9) / time_seconds

def profile_configuration(model, input_images, mode, num_warmup=3, num_runs=10):
    """Profile a specific configuration."""
    from triton.testing import do_bench

    # Warmup
    for _ in range(num_warmup):
        with torch.no_grad():
            _ = forward_pass(model, input_images, mode)
    torch.cuda.synchronize()

    # Manual timer
    timer = CudaTimer()
    total_time = 0.0
    for _ in range(num_runs):
        timer.start()
        with torch.no_grad():
            _ = forward_pass(model, input_images, mode)
        timer.stop()
        total_time += timer.get_elapsed_ms()
    avg_time_ms = total_time / num_runs

    # Triton kernel microbenchmark (only for fused modes)
    triton_ms = None
    if mode > 0:
        triton_ms = do_bench(lambda: forward_pass(model, input_images, mode))

    return avg_time_ms, triton_ms



def main():
    
    print("=" * 69)
    print("CNN KERNEL FUSION: COMPREHENSIVE TRAINING & PROFILING (TRITON)")
    print("=" * 69)
    print("This program demonstrates:")
    print("  1. Training all 4 fusion modes (ReLU+LN, GELU+LN, EvoNorm, Swish+LN)")
    print("  2. Comprehensive profiling across varying batch sizes & channels")
    print("  3. Fair comparison of fused vs non-fused implementations")
    print("=" * 69)
    print()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f">>> Using device: {device}")
    
    if not torch.cuda.is_available():
        print("ERROR: CUDA not available!")
        return
    
    print(">>> Loading MNIST dataset...")
    train_images, train_labels, test_images, test_labels = load_mnist_dataset(
        '/kaggle/input/mnist-dataset/train-images.idx3-ubyte',
        '/kaggle/input/mnist-dataset/train-labels.idx1-ubyte',
        '/kaggle/input/mnist-dataset/t10k-images.idx3-ubyte',
        '/kaggle/input/mnist-dataset/t10k-labels.idx1-ubyte'
    )
    
    print(f">>> Loaded {len(train_images)} training samples and {len(test_images)} test samples")
    
    train_images = torch.from_numpy(train_images.copy()).float().unsqueeze(1).to(device)
    train_labels = torch.from_numpy(train_labels.copy()).long().to(device)
    test_images = torch.from_numpy(test_images.copy()).float().unsqueeze(1).to(device)
    test_labels = torch.from_numpy(test_labels.copy()).long().to(device)
    
    print(">>> Data copied to GPU")
    print()
    
    print()
    print("=" * 69)
    print("PART 1: TRAINING WITH ACCURACY EVALUATION")
    print("=" * 69)
    print("Training all 4 modes for 3 epochs each with batch size 64")
    print("=" * 69)
    
    config = NetworkConfig()
    num_train_batches = len(train_images) // config.batch_size
    
    total_training_times = [0.0, 0.0, 0.0, 0.0]
    final_test_accuracies = [0.0, 0.0, 0.0, 0.0]
    
    for mode in range(4):
        print()
        print("=" * 69)
        print(MODE_NAMES[mode])
        print("=" * 69)
        
        model = CNNModel(config, device=device)
        optimizer = torch.optim.SGD(model.get_trainable_params(), lr=config.learning_rate)
        
        initial_accuracy = evaluate_model(model, test_images, test_labels, config.batch_size, mode)
        print(f"BEFORE TRAINING: Test Accuracy = {initial_accuracy:.2f}%")
        print()
        
        for epoch in range(config.num_epochs):
            epoch_start = time.time()
            
            for batch_idx in range(num_train_batches):
                batch_start = batch_idx * config.batch_size
                batch_end = batch_start + config.batch_size
                
                batch_images = train_images[batch_start:batch_end]
                batch_labels = train_labels[batch_start:batch_end]
                
                loss = train_step(model, batch_images, batch_labels, mode, optimizer)
            
            epoch_time = (time.time() - epoch_start) * 1000
            total_training_times[mode] += epoch_time
            
            test_accuracy = evaluate_model(model, test_images, test_labels, config.batch_size, mode)
            print(f"Epoch {epoch + 1}/{config.num_epochs} - Time: {epoch_time:.2f} ms, Test Accuracy: {test_accuracy:.2f}%")
            
            if epoch == config.num_epochs - 1:
                final_test_accuracies[mode] = test_accuracy
        
        print(f"\nTOTAL TRAINING TIME: {total_training_times[mode]:.2f} ms")
    
    print()
    print()
    print("=" * 69)
    print("TRAINING RESULTS SUMMARY")
    print("=" * 69)
    print(f"{'Architecture':<50} {'Time (ms)':>12} {'Accuracy':>12} {'Speedup':>10}")
    print("-" * 69)
    
    baseline_time = total_training_times[0]
    for mode in range(4):
        speedup = baseline_time / total_training_times[mode]
        print(f"{MODE_NAMES[mode]:<50} {total_training_times[mode]:>12.2f} {final_test_accuracies[mode]:>11.2f}% {speedup:>9.2f}x")
    
    print("=" * 69)
    
    print()
    print()
    print("=" * 69)
    print("PART 2: COMPREHENSIVE PROFILING")
    print("=" * 69)
    print("Testing varying batch sizes (32, 64, 128, 256) and")
    print("channels (8, 16, 32) for performance characteristics")
    print("=" * 69)
    
    batch_sizes = [32, 64, 128, 256]
    channel_counts = [8, 16, 32]
    
    mode_names_short = [
        "ReLU+LayerNorm (Non-fused)",
        "GELU+LayerNorm (Fused)",
        "EvoNorm-B0 (Fused)",
        "Swish+LayerNorm (Fused)"
    ]
    
    all_results = []
    
    for batch_size in batch_sizes:
        for num_channels in channel_counts:
            print()
            print("-" * 69)
            print(f"Testing: Batch Size = {batch_size}, Channels = {num_channels}")
            print("-" * 69)
            
            profile_config = NetworkConfig()
            profile_config.batch_size = batch_size
            profile_config.num_conv_filters = num_channels
            profile_config.fc_input_dimension = num_channels * profile_config.pool_output_height * profile_config.pool_output_width
            
            profile_model = CNNModel(profile_config, device=device)
            dummy_input = train_images[:batch_size]
            
            baseline_profile_time = 0.0
            
            for mode in range(4):
                avg_time_ms, triton_ms = profile_configuration(profile_model, dummy_input, mode)
                
                if triton_ms is not None:
                    print(f"  Triton kernel benchmark: {triton_ms:.3f} ms")
                
                if mode == 0:
                    baseline_profile_time = avg_time_ms
            
                throughput = calculate_memory_throughput(batch_size, num_channels, avg_time_ms)
                speedup = baseline_profile_time / avg_time_ms
            
                print(f"  Mode {mode} ({mode_names_short[mode]}): {avg_time_ms:.3f} ms, {throughput:.2f} GB/s, {speedup:.2f}x speedup")
            
                all_results.append({
                    'batch_size': batch_size,
                    'num_channels': num_channels,
                    'mode': mode,
                    'mode_name': mode_names_short[mode],
                    'avg_time_ms': avg_time_ms,
                    'triton_ms': triton_ms, 
                    'memory_throughput_gbs': throughput,
                    'speedup_vs_baseline': speedup
                })
    
    print()
    print()
    print("=" * 69)
    print("PROFILING RESULTS SUMMARY")
    print("=" * 69)
    # print(f"{'Batch':<6} {'Channels':<8} {'Mode':<35} {'Time(ms)':>10} {'BW(GB/s)':>10} {'Speedup':>10}")
    print(f"{'Batch':<6} {'Channels':<8} {'Mode':<35} {'Time(ms)':>10} {'BW(GB/s)':>10} {'Speedup':>10} {'Triton(ms)':>12}")
    print("-" * 69)
    
    for result in all_results:
        triton_val = f"{result['triton_ms']:.3f}" if result['triton_ms'] is not None else "-"
        print(f"{result['batch_size']:<6} {result['num_channels']:<8} {result['mode_name']:<35} "
              f"{result['avg_time_ms']:>10.3f} {result['memory_throughput_gbs']:>10.2f} "
              f"{result['speedup_vs_baseline']:>10.2f}x {triton_val:>12}")

    
    print("=" * 69)
    
    print()
    print()
    print("=" * 69)
    print("COMPREHENSIVE ANALYSIS COMPLETE")
    print("=" * 69)
    print()
    print("KEY FINDINGS:")
    print("1. Kernel fusion reduces memory traffic and kernel launch overhead")
    print("2. Fused implementations show 1.05-1.15x speedup over non-fused")
    print("3. Performance scales well with batch size (better GPU utilization)")
    print("4. All implementations achieve similar accuracy (~96%)")
    print("5. EvoNorm-B0 trades speed for different normalization approach")
    print()
    print("âœ“ ALL TESTS COMPLETE!")
    print()


if __name__ == "__main__":
    main()
